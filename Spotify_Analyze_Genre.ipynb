{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spotify_Analyze_Genre.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yxynj67aT9Xq"
      },
      "source": [
        "#Analyze Spotify Genre\n",
        "##In this notebook, I fed the data into a multi-class classification algorithm to predict the genre of a song based on the audible attributes. For the purpose of limiting the genres to classify, I filtered for songs within just the top 7 most popular genres according to the genre analysis done in previous sections. \n",
        "\n",
        "##With some optimizations, the algorithm was able to predict a songs genre with about 80% accuracy in 1 prediction and with about 89% accuracy with 2 predictions. I was extremely satisfied with this result given the difficulty of 7-way classification, especially considering that I started with an accuracy of around 58% before model optimization and data changes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJD7k5KXboZS"
      },
      "source": [
        "## Data Preprocessing:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlnmJvKn2WiY"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcgrCxCO2vyM"
      },
      "source": [
        "track_table = pd.read_csv(\"cleaned_tracks_both.csv\")\n",
        "track_table.dropna(inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT4Ypz7uXwQ3"
      },
      "source": [
        "A quick reminder about what genres we're trying to classify and the the number of tracks per genre:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWRXVENGU83A"
      },
      "source": [
        "track_table.groupby(\"master_popular_genre\").track_id.count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gkiZCar4Wmh"
      },
      "source": [
        "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "data_numerics = pd.concat([track_table.select_dtypes(include=[np.number]),track_table['master_popular_genre']],axis=1, sort=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50PsH8ALACJ8"
      },
      "source": [
        "#move genre to front\n",
        "track_table_dummies = pd.get_dummies(data_numerics,columns=['key','time_signature',],drop_first=True)\n",
        "track_table_dummies = track_table_dummies.drop('loudness', axis=1)\n",
        "track_table_dummies.head()\n",
        "genre = track_table_dummies['master_popular_genre']\n",
        "track_table_dummies.drop('master_popular_genre', axis=1,inplace = True)\n",
        "track_table_dummies.insert(0, 'master_popular_genre', genre)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwKGxy9jX17Y"
      },
      "source": [
        "One more look at the data we're using to train the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5tk5mj4AtlX"
      },
      "source": [
        "track_table_dummies.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itSs59Ct9Jj5"
      },
      "source": [
        "X = track_table_dummies.iloc[:,1:]\n",
        "y = track_table_dummies.iloc[:,0]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPeTRM4caLjj"
      },
      "source": [
        "## Training the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oURyx-jh74-m"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "genre_order = ['country', 'hiphop', 'house', 'indie', 'pop', 'r&b', 'rock']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNVK_ZFToRX9"
      },
      "source": [
        "(Creating a few helper methods for printing model results before building the models)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnFrEBKxdD7k"
      },
      "source": [
        "def print_accuracy(genre_index, conf_matrix):\n",
        "  print(\"Accuracy predicting\", genre_order[genre_index], \":\", conf_matrix[genre_index,genre_index]/(sum(conf_matrix[genre_index,:])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXVOUzmMM8KT"
      },
      "source": [
        "def print_grid_results(grid, x_test, y_test):\n",
        "  conf_matrix = confusion_matrix(y_test, grid.predict(X_test), labels=genre_order)\n",
        "  print(\"The best score is {}\".format(grid.best_score_))\n",
        "  print(\"The best hyper parameter setting is {}\".format(grid.best_params_))\n",
        "  print(\"Model Accuracy:\", accuracy_score(y_test,grid.predict(X_test)))\n",
        "  print()\n",
        "  for i in range(0,len(genre_order)):\n",
        "    print_accuracy(i,conf_matrix)\n",
        "  fig, ax = plt.subplots(figsize=(12,10)) \n",
        "  sns.heatmap(conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis], annot=True,fmt='.2%', cmap='Blues',xticklabels=genre_order,yticklabels=genre_order, ax=ax)\n",
        "  ax.set(xlabel='Predicted Genre', ylabel='True Genre')\n",
        "  ax.set_title(\"Proportional Genre Confusion Matrix\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sssr7jBkod2Y"
      },
      "source": [
        "### Model Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--YOI1d6oWCm"
      },
      "source": [
        "Piggybacking off the success of the Gradient Boost from the previous exploration, I initially tried using a Gradient Boost classifier for the multi-class classification. However, the Gradient Boost it took 5-10 minutes per iteration. With cross-validation and a grid search of 3-4 different variables, this optimization took hours and yielded a poor testing accuracy of around 58%.\n",
        "\n",
        "From there I trained several other models using different classifiers like logistic regressions, SVMs, different boosting methods, and tree methods. I found random forest to be by far the most accurate in its performance, so I started with just a rudimentary Random Forest model with 500 estimators."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxHJrsX4S2K6"
      },
      "source": [
        "X = track_table_dummies.iloc[:,1:]\n",
        "y = track_table_dummies.iloc[:,0]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "\n",
        "rf_param_grid = {\n",
        "       #'n_estimators': range(350, 700, 100),\n",
        "       'n_estimators': [500]\n",
        "}\n",
        "grid_rf = GridSearchCV(RandomForestClassifier(), rf_param_grid, cv=3, verbose=2).fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDW4X_emo4bk"
      },
      "source": [
        "Let's look at a confusion matrix of how this crude Random Forest model predicted our genres and how the model performed overall:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W69ak9o_4KYV"
      },
      "source": [
        "conf_matrix = confusion_matrix(y_test, grid_rf.predict(X_test), labels=genre_order)\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "print(plot_confusion_matrix(X=X_test,y_true=y_test, labels=genre_order,estimator=grid_rf, ax=ax,values_format = 'd'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1eBXgXepX07"
      },
      "source": [
        "Based on the above confusion matrix, we can see a few things:\n",
        "\n",
        "1. Pop understandbly dominates the dataset. This somewhat obscures the confusion matrix because there are so many more pop rows than any other genre. We can see that based on the vertical light shade on the pop column (,4), our model is just blindly predicting pop in many cases where it's unsure. This poses a bit of an issue that I address on later.\n",
        "2. Conversely, R&B is significantly underrepresented in the dataset. Only 769 tracks are predicted correctly as R&B, while 532 R&B tracks are incorrectly labeled as pop.\n",
        "3. While the absolute number of tracks predicted in each square of the confusion matrix is interesting, it would be more useful to see the proportion of true tracks in each square. For example, I want to see the percentage of true R&B tracks that are predicted as R&B, rather than the absolute number of tracks, so the matrix is more balanced. Thus in the following block, I outputted the accuracy scores as well as the proportional confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_s7rqXfWpZLt"
      },
      "source": [
        "print_grid_results(grid_rf,X_test,y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Syn_AZ4Xqy-p"
      },
      "source": [
        "The rudamentary RF model gives us a baseline performance of 68% in predicting the correct genre of a track.\n",
        "\n",
        "Additionally, this proportional confusion matrix is a bit more readable. Additionally, this further emphasizes the model's tendancy to overpredict pop due to its prevalence in the dataset. In fact, when we look at what genre is most often miscategorized for each track, we see the"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-U3rTrBIU1O"
      },
      "source": [
        "for count,genre_preds in enumerate(conf_matrix):\n",
        "  print(\"True Genre:\", genre_order[count])\n",
        "  genre_preds[list(genre_preds).index(max(genre_preds))] = 0\n",
        "  incorrect_guess = genre_order[list(genre_preds).index(max(genre_preds))]\n",
        "  print(\"Most Common Incorrect Prediction:\", incorrect_guess)\n",
        "  print(\"Percent of All Incorrect Predictions as\", incorrect_guess, (max(genre_preds) / sum(genre_preds)))\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElY5nrS7rOAk"
      },
      "source": [
        "Clearly, pop is dominating the dataset and overpowering the importance of less frequently occuring genres like R&B and house. Let's address that..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVY4tGR9rVXF"
      },
      "source": [
        "## Balancing the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQsyAPmWf8kt"
      },
      "source": [
        "Before optimizing the model's hyperparameters, I wanted to try to even out the distribution of tracks of each genre. I chose to test several outcomes and monitor how the model performed with each:\n",
        "1. Only downsampling pop tracks\n",
        "2. Only upsampling lesser occuring genres and leaving pop tracks as they are\n",
        "3. Downsampling pop tracks AND upsampling lesser occuring genres"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4uGZLVSr2P2"
      },
      "source": [
        "### 1. Downsampling pop to 10,000 tracks\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7nXAN696jzl"
      },
      "source": [
        "from sklearn.utils import resample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZpmH_5wgHvj"
      },
      "source": [
        "df_nonpop = track_table_dummies[track_table_dummies['master_popular_genre']!='pop']\n",
        "df_pop = track_table_dummies[track_table_dummies['master_popular_genre']=='pop']\n",
        "samp = 10000\n",
        "# Downsample majority class\n",
        "df_pop_downsampled = resample(df_pop, \n",
        "                                 replace=False,    # sample without replacement\n",
        "                                 n_samples=10000) # reproducible results\n",
        " \n",
        "# Combine minority class with downsampled majority class\n",
        "df_pop_downsampled = pd.concat([df_pop_downsampled, df_nonpop])\n",
        " \n",
        "# Display new class counts\n",
        "df_pop_downsampled.master_popular_genre.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taLYXyCQFshY"
      },
      "source": [
        "X = df_pop_downsampled.iloc[:,1:]\n",
        "y = df_pop_downsampled.iloc[:,0]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "\n",
        "rf_param_grid = {\n",
        "       #'n_estimators': range(350, 700, 100),\n",
        "       'n_estimators': [650]\n",
        "}\n",
        "grid_rf_under = GridSearchCV(RandomForestClassifier(), rf_param_grid, cv=3, verbose=2).fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43hPBadgOmUX"
      },
      "source": [
        "print(\"Undersampling pop tracks:\")\n",
        "print_grid_results(grid_rf_under,X_test,y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7qodvI6sTsh"
      },
      "source": [
        "While thye confusion matrix looks a bit more balanced and there isn't a strong overprediction in the pop genre, the accuracy did not improve much -- it's still 68%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCCOPxLzH3FS"
      },
      "source": [
        "### 2. Upsampling r&b, house, indie, and rock to 10,000 tracks\n",
        "Upsampling these genres added 1000, 2500, 3500, and 4500 to the rock, indie, house, and r&b genres respectively, to bring them all to 10000 tracks each."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1bjs8VhLfmU"
      },
      "source": [
        "def oversample(frame, genre, numTracks):\n",
        "  df = frame[frame['master_popular_genre']==genre]\n",
        "  df_upsampled = resample(df, replace=True, n_samples=numTracks)\n",
        "  return df_upsampled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkE7H-TKH5Kd"
      },
      "source": [
        "samp = 10000\n",
        "\n",
        "df_rb = oversample(track_table_dummies,\"r&b\", samp)\n",
        "df_house = oversample(track_table_dummies,\"house\", samp)\n",
        "df_indie = oversample(track_table_dummies,\"indie\", samp)\n",
        "df_rock = oversample(track_table_dummies,\"rock\", samp)\n",
        "\n",
        "df_rest = track_table_dummies[track_table_dummies['master_popular_genre']!='r&b']\n",
        "df_rest = df_rest[df_rest['master_popular_genre']!='house']\n",
        "df_rest = df_rest[df_rest['master_popular_genre']!='indie']\n",
        "df_rest = df_rest[df_rest['master_popular_genre']!='rock']\n",
        "\n",
        "# Combine minority class with downsampled majority class\n",
        "df_upsampled = pd.concat([df_rest, df_rb, df_house, df_indie, df_rock])\n",
        " \n",
        "# Display new class counts\n",
        "df_upsampled.master_popular_genre.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3c5PhvsF-24"
      },
      "source": [
        "X = df_upsampled.iloc[:,1:]\n",
        "y = df_upsampled.iloc[:,0]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "\n",
        "rf_param_grid = {\n",
        "       #'n_estimators': range(350, 700, 100),\n",
        "       'n_estimators': [650]\n",
        "}\n",
        "grid_rf_up = GridSearchCV(RandomForestClassifier(), rf_param_grid, cv=3, verbose=2).fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLMqmA5xOxFY"
      },
      "source": [
        "print(\"Oversampling less frequent genres:\")\n",
        "print_grid_results(grid_rf_up,X_test,y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnIvqhydwqLR"
      },
      "source": [
        "Wow! With only upsampling the less common genres, the accuracy of the model jumps all the way to nearly 80%. This is a huge improvement from the 68% test accuracy of the original model on the imbalanced dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaVZM7s8mNNb"
      },
      "source": [
        "## 3. Downsampling and upsampling all genres to ~10000 tracks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ly-rgmaxmRMP"
      },
      "source": [
        "df_nonpop = df_upsampled[df_upsampled['master_popular_genre']!='pop']\n",
        "df_pop = df_upsampled[df_upsampled['master_popular_genre']=='pop']\n",
        "samp = 10000\n",
        "# Downsample majority class\n",
        "df_downsampled = resample(df_pop, \n",
        "                                 replace=False,    # sample without replacement\n",
        "                                 n_samples=10000) # reproducible results\n",
        " \n",
        "# Combine minority class with downsampled majority class\n",
        "df_both = pd.concat([df_downsampled, df_nonpop])\n",
        " \n",
        "# Display new class counts\n",
        "df_both.master_popular_genre.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cg2oeGjuGKOp"
      },
      "source": [
        "X = df_both.iloc[:,1:]\n",
        "y = df_both.iloc[:,0]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "\n",
        "rf_param_grid = {\n",
        "       #'n_estimators': range(350, 700, 100),\n",
        "       'n_estimators': [650]\n",
        "}\n",
        "grid_rf_balanced = GridSearchCV(RandomForestClassifier(), rf_param_grid, cv=3, verbose=2).fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tn6Da7_TO3A9"
      },
      "source": [
        "print(\"Undersampling pop tracks and oversampling uncommon genres:\")\n",
        "print_grid_results(grid_rf_balanced,X_test,y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TbDypUk2Bl8"
      },
      "source": [
        "Combining both upsampling and downsampling surprisingly lowers the accuracy slightly overall, but we can see that the predictions are a bit more balanced. The model does not default to pop as often, but the accuracy in predicting true pop trakcs clearly suffered."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZyGoMjq2CPm"
      },
      "source": [
        "##Fully tuning the hyperparameters with the balanced dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQyg_63-6vuk"
      },
      "source": [
        "Now that the dataset is better balanced, I wanted to leverage a grid search to tune the hyperparameters of the random forest model to improve performance as much as possible.\n",
        "\n",
        "One thing to consider here is which of the above datasets to use to train the final model. There are really only 2 options of what to use, as the first method of only undersampling pop was significantly less accurate. The following two had roughly the same test accuracy (~80%).\n",
        "* Method #2, in which underrepresented genres were upsampled\n",
        "* Method #3, in which underrepresented genres were upsampled AND pop was downsampled\n",
        "\n",
        "The benefits of using Method #2 is that the model has been trained on a higher proportion of pop relative to the other tracks. Given the prevalance of pop music in general, one might consider this a more accurate representation of the data that this model would typically see if given a real-world sample of tracks on Spotify. Because there are more pop tracks than any other, pop tracks would be more likely to occur. This model would be trained under similar circumstances. So if the use case of the model was to intake random tracks on Spotify to categorize them on genre, this approach would make sense.\n",
        "\n",
        "The benefits of using Method #3 is that this model likely has a better pure understanding of what audible tracks make up each genre, besides pop. Because it was fed a roughly equal number of tracks of each genre, it won't have the same biases toward pop as method #2. However, as visualized by the confusion matrix in the method 2 section, the model is fairly poor at predicting pop tracks successfully, yielding only a 48% accuracy compared to around 70% from Method #2. However, since it's more accurate in every other other genre, the overall accuracy is about the same. If the goal is to build a model that can take any 1 track, equally likely to be of any genre, this model will typically perform better. However, if fed a real-world distribution of Spotify tracks, it would likely perform worse given the prevalence of pop.\n",
        "\n",
        "I chose method #3 to build the model, as my goal was to most accurately predict a single song's genre without considering the liklihood that a randomly selected song would be pop. I think the approach of balancing all input tracks to around 10,000 makes the most sense for this purpose"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAgbryM7Q8B5"
      },
      "source": [
        "X = df_both.iloc[:,1:]\n",
        "y = df_both.iloc[:,0]\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "\n",
        "rf_param_grid = {\n",
        "          #'bootstrap': [True, False],\n",
        "          'bootstrap': [False],\n",
        "          # 'max_depth': [10, 50, 100, None],\n",
        "          'max_depth': [100],\n",
        "          #'max_features': ['auto', 'sqrt'],\n",
        "          'max_features': ['auto'],\n",
        "          #'n_estimators': [600, 1000, 1400, 1800, 2000]\n",
        "          'n_estimators': [1400]\n",
        "}\n",
        "grid_rf_opt = GridSearchCV(RandomForestClassifier(), rf_param_grid, cv=5).fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYXTCKd0_KFp"
      },
      "source": [
        "print(\"Undersampling pop tracks and oversampling uncommon genres:\")\n",
        "print_grid_results(grid_rf_opt,X_test,y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBEBQfHn1CYo"
      },
      "source": [
        "After running the grid search to tune each hyperparameter, performance barely improved. Performance predicting pop did improve to about 52%, however, overall accuracy is still about 80%. Regardless, improving from a 58% Gradient Boost, to a 68% Random Forest, to an 80% tuned random forest is still great improvement and is a satisfying result for a 7-way genre classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdaAHENpbRSz"
      },
      "source": [
        "##Let's see some predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DE0zGFuMPRws"
      },
      "source": [
        "Now, let's analyze the performance a bit more and take a look at what tracks the model is right and wrong about. We can even listen to some of them on Spotify and see how we would classify them ourselves."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNrp5AtPKlYO"
      },
      "source": [
        "preds = []\n",
        "for prediction in grid_rf_opt.predict_proba(X_test):\n",
        "  pred_ind = []\n",
        "  prediction = list(prediction)\n",
        "  pred_ind.append(max(prediction))\n",
        "  pred_ind.append(prediction.index(max(prediction)))\n",
        "  preds.append(pred_ind)\n",
        "\n",
        "for pred in preds:\n",
        "  pred[1] = genre_order[pred[1]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XT5-W-KOGqm"
      },
      "source": [
        "in_list = []\n",
        "for i in range(0,len(preds)):\n",
        "  in_list.append(int(list(y_test)[i] == preds[i][1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBzFjZZdQkbj"
      },
      "source": [
        "prediction_conf = []\n",
        "predicted_genre = []\n",
        "\n",
        "for i in range(0,len(y_test)):\n",
        "  prediction_conf.append(preds[i][0])\n",
        "  predicted_genre.append(preds[i][1])\n",
        "\n",
        "\n",
        "prediction_frame = pd.DataFrame({\"Prediction Confidence\":prediction_conf,\n",
        "                                 \"Predicted Correctly\":in_list,\n",
        "                                 \"True Genre\": y_test,\n",
        "                                 \"Predicted Genre\": predicted_genre,\n",
        "                                 \"Track Name\": track_table.track_name[y_test.index],\n",
        "                                 \"Artist Name\": track_table.art_name[y_test.index]})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4D_keT2tRK2y"
      },
      "source": [
        "Let's first look at what songs the model was confident in. There are many songs that the RF outputted a predicted probability of 1, meaning that the model was very confident in these predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ikq0bdyRbAO"
      },
      "source": [
        "prediction_frame.drop_duplicates().sort_values(by=\"Prediction Confidence\", ascending=False).head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhWI9-iBRUly"
      },
      "source": [
        "Here is the track by Jorja Smith that the model correctly predicted as R&B: [The One - Jorja Smith](https://open.spotify.com/track/1Ahp4PZ1vzdbzBCedUrsqI)\n",
        "\n",
        "Here is a track by George Strait that the model correctly predicted as Country: [Easy Come, Easy Go - George Strait](https://open.spotify.com/track/0hqXlHVE94CTwXXWRdikbY?si=e9e81cb391b24512)\n",
        "\n",
        "These both make sense, as they sound quintessentially R&B and country. More interesting will be looking at what tracks the model predicted *incorrectly*. Let's take a look at a few of those."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "512KOR30Trl7"
      },
      "source": [
        "prediction_frame[prediction_frame['Predicted Correctly'] == 0].drop_duplicates().sort_values(by=\"Prediction Confidence\", ascending=False).head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuyQbFQKTcLo"
      },
      "source": [
        "Unsurprisingly, a ton of these missed predictions are true-genre pop. This is clearly the model's worst category at predicting, thought this was expected after we went with the Method #2 approach. Let's listen to a few of these in particular:\n",
        "\n",
        "The first one in the list sounds a lot like house to me, despite its genre label being pop: [J'ai Envie De Toi - Armin van Buuren](https://open.spotify.com/track/1Foo16rQ7mTzEk2Fb0CIOv?si=935e925f56eb4554) It makes sense why the model would have chosen house, as there are no lyrics and it has tons of elements of house music.\n",
        "\n",
        "Let's look at a track that isn't true-genre pop: [Tee Pees 1-12 - Father John Misty](https://open.spotify.com/track/0iOtvXw6nRQmtUBiZm9YY6?si=71ff0f491e83441c) Again, no surprise here! The song has a number of country elements like the violin and tempo. This is another case where I would arguably side more with the classification than with the \"True Genre\", although the line between indie and country can certainly be fuzzy at times.\n",
        "\n",
        "Finally, howabout the last row above, in which the model predicted a rock track as house -- that seems like an unexpected confusion: [Pharaohs & Pyramids - Cut Copy](https://open.spotify.com/track/5KtZsj1eaI4uZBBYUOw3zC?si=2807bf8b32fc4540). Honestly, this is a pretty unusual song. I imagine if we allowed more genres, this may fall under some techno category, as it is certainly a mix of house and pop elements. Again, though, I don't quite see this as rock, so perhaps a lot of these errors are due to the erroneous labeling of the true values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pv6AT1-_L3_7"
      },
      "source": [
        "#Predict 2 Genres\n",
        "As a last little experiment, I wanted to see how accurate the model would be if it was able to make 2 predictions for a track -- the 2 genres which it views as the most likely for that track."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFNNttCe9JE1"
      },
      "source": [
        "# Add prediction of 2nd highest probability genre\n",
        "two_preds = []\n",
        "for prediction in grid_rf_opt.predict_proba(X_test):\n",
        "  prediction = list(prediction)\n",
        "  two_maxes = []\n",
        "  two_maxes.append(prediction.index(max(prediction)))\n",
        "  prediction[prediction.index(max(prediction))] = 0\n",
        "  two_maxes.append(prediction.index(max(prediction)))\n",
        "  two_preds.append(two_maxes)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_FSe5aACDQo"
      },
      "source": [
        "# Convert probability to genre name\n",
        "for pred in two_preds:\n",
        "  pred[0] = genre_order[pred[0]]\n",
        "  pred[1] = genre_order[pred[1]]"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1yRLa6BKUIP"
      },
      "source": [
        "# Create list to evaluate accuracy\n",
        "in_list = []\n",
        "for i in range(0,len(two_preds)):\n",
        "  in_list.append(int(list(y_test)[i] in two_preds[i]))"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsOWRCPTLmDo",
        "outputId": "2933412f-24a8-4e2a-db4a-f59da12c4eed"
      },
      "source": [
        "print(\"Accuracy:\" , sum(in_list) / len(in_list))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8898551966412476\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRg0CDvbV9Ba"
      },
      "source": [
        "With 2 predictions, the model improves about 10% and has an 89% accuracy. That's pretty awesome, considering there are 7 different genres to consider for this multiclass labeling. Pretty cool! I wonder what ~11% of tracks are mislabeled here too. These would be tracks where the model really missed badly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sTdSCGeMOB5"
      },
      "source": [
        "pred_correctly = []\n",
        "true_genre = []\n",
        "predicted_genre = []\n",
        "\n",
        "for i in range(0,len(y_test)):\n",
        "  pred_correctly.append(in_list[i])\n",
        "  true_genre.append(list(y_test)[i])\n",
        "  predicted_genre.append(two_preds[i])\n",
        "\n",
        "prediction_frame = pd.DataFrame({\"Predicted Correctly\":pred_correctly,\n",
        "                                 \"True Genre\": true_genre,\n",
        "                                 \"Predicted Genres\": predicted_genre,\n",
        "                                 \"Track Name\": track_table.track_name[y_test.index],\n",
        "                                 \"Artist Name\": track_table.art_name[y_test.index]})"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "tvFTh_EJPZoe",
        "outputId": "ab3b16c9-18c9-4e11-a631-fa96d84ca501"
      },
      "source": [
        "display(prediction_frame[prediction_frame[\"Predicted Correctly\"] == 0])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Predicted Correctly</th>\n",
              "      <th>True Genre</th>\n",
              "      <th>Predicted Genres</th>\n",
              "      <th>Track Name</th>\n",
              "      <th>Artist Name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>16294</th>\n",
              "      <td>0</td>\n",
              "      <td>pop</td>\n",
              "      <td>[hiphop, r&amp;b]</td>\n",
              "      <td>Comb</td>\n",
              "      <td>Skizzy Mars</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28261</th>\n",
              "      <td>0</td>\n",
              "      <td>house</td>\n",
              "      <td>[rock, pop]</td>\n",
              "      <td>Miracle</td>\n",
              "      <td>Madeon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64738</th>\n",
              "      <td>0</td>\n",
              "      <td>rock</td>\n",
              "      <td>[country, hiphop]</td>\n",
              "      <td>Take a Sip</td>\n",
              "      <td>Skrizzly Adams</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16204</th>\n",
              "      <td>0</td>\n",
              "      <td>hiphop</td>\n",
              "      <td>[pop, house]</td>\n",
              "      <td>Salute</td>\n",
              "      <td>Future</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31629</th>\n",
              "      <td>0</td>\n",
              "      <td>indie</td>\n",
              "      <td>[hiphop, pop]</td>\n",
              "      <td>Devil Don't You Fool Me</td>\n",
              "      <td>Josh Farrow</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15727</th>\n",
              "      <td>0</td>\n",
              "      <td>hiphop</td>\n",
              "      <td>[house, indie]</td>\n",
              "      <td>Boogieman</td>\n",
              "      <td>Childish Gambino</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66194</th>\n",
              "      <td>0</td>\n",
              "      <td>pop</td>\n",
              "      <td>[house, rock]</td>\n",
              "      <td>Wild Life</td>\n",
              "      <td>OneRepublic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27208</th>\n",
              "      <td>0</td>\n",
              "      <td>house</td>\n",
              "      <td>[pop, country]</td>\n",
              "      <td>Light Me Up</td>\n",
              "      <td>RL Grime</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>719</th>\n",
              "      <td>0</td>\n",
              "      <td>country</td>\n",
              "      <td>[pop, rock]</td>\n",
              "      <td>Remind Me (with Carrie Underwood)</td>\n",
              "      <td>Brad Paisley</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27841</th>\n",
              "      <td>0</td>\n",
              "      <td>house</td>\n",
              "      <td>[r&amp;b, indie]</td>\n",
              "      <td>Slow Me Down</td>\n",
              "      <td>Mauve</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2571 rows Ã— 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Predicted Correctly  ...       Artist Name\n",
              "16294                    0  ...       Skizzy Mars\n",
              "28261                    0  ...            Madeon\n",
              "64738                    0  ...    Skrizzly Adams\n",
              "16204                    0  ...            Future\n",
              "31629                    0  ...       Josh Farrow\n",
              "...                    ...  ...               ...\n",
              "15727                    0  ...  Childish Gambino\n",
              "66194                    0  ...       OneRepublic\n",
              "27208                    0  ...          RL Grime\n",
              "719                      0  ...      Brad Paisley\n",
              "27841                    0  ...             Mauve\n",
              "\n",
              "[2571 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5E5qKWPjWdM3"
      },
      "source": [
        "Let's check some of these out:\n",
        "*   [Take a Sip - Skrizzly Adams](https://open.spotify.com/track/3zgzbGes5o5X0ExPhj0zpl?si=49562c0c03cc47c4) - This one makes sense as a country/hiphop track, as it's a bit of a fusion between rap and country rock\n",
        "*   [Salute - Future](https://open.spotify.com/track/1tjpoAROSHmr9QLb7Ibqoq?si=8b03d220e0a74389) - Not really sure what happened on this one. This is about as hiphop as you can get with the trap drums and rapping. This was a pretty bad miss. The house prediction was especially confusing.\n",
        "*   [Boogieman - Childish Gambino](https://open.spotify.com/track/0SunFlwqT44E0BU0yrgM7u?si=5c52a9b7638143e7) - This one makes sense as just an extremely difficult song to predict. I would personally call this funk, perhaps? Of our genre's, I think hiphop is a fair true labell, but I'm not surprised the model missed this, given its unconventional sound.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kxl22eEcxRr"
      },
      "source": [
        "## How to improve the model in the future\n",
        "It seems that the best place to focus on for future improvement of the genre predicting model would be to improve the true labeling of the tracks. Detailed in part 1, every track / artist outputted by the Spotify API has a number of genre tags, but there is not a single definitive genre for one track. To allow us to build a genre-predicting algorithm, the track genre tags were pooled into a dictionary and I chose the most occuring gender to determine the True Genre for that track. This is somewhat imperfect and likely lead to some faulty labeling of true genre. However, this was the best option available until Spotify labels the genre of the track themselves, or I come up with some other NLP improvements for the tag text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lAbQXyVdnqf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}